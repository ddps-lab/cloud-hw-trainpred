{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd \n",
    "import pickle\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import io\n",
    "import math\n",
    "import hw_functions\n",
    "import features\n",
    "\n",
    "model_list = ['MNIST_CNN*','AlexNet*','CIFAR10*','InceptionV3*',\n",
    "                     'VGG19*','ResNet50*','InceptionResNetV2*','LeNet5*',\n",
    "                   'ResNetSmall*']\n",
    "Instances =  [ 'p100' ,'p3', 'g4dn', 'g5' ]\n",
    "columns_name = ['Train_Name','Instances','model','batchsize','datasize','totaldata','total_latency_second','one_epoch_average_second'] + features.describe_columns + features.not_move_value_columns \n",
    "\n",
    "df = pd.DataFrame(columns = columns_name)  # 파일하나당 한줄씩 저장해줄것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for Instance_name in Instances:  # 인스턴스별로\n",
    "    print(Instance_name)\n",
    "    Instance = '../DataSet/RawData/GPU/'+Instance_name\n",
    "    \n",
    "    # DATA.csv 전처리\n",
    "    Data_T = pd.read_csv(Instance+'/Data.csv')\n",
    "    Data_T ,Data_new_timestamp = hw_functions.Datacsv_preprocess(Data_T)\n",
    "    Data_T= Data_T[Data_T.columns.difference(['hour', 'minute', 'second' , 'millisecond' , 'hms'])]\n",
    "    Data_T_columns = [c for c in Data_T.columns if c.lower()[:7] != 'retired']      # retired_pages.double_bit.count/retired_pages.single_bit_ecc.count [N/A], 0 으로 인스턴스별로 다르게 출력됨\n",
    "    Data_T_columns = [c for c in Data_T_columns if c.lower()[:3] != 'ecc']\n",
    "    Data_T = Data_T[Data_T_columns]\n",
    "    new_columns = []\n",
    "    for c in Data_T.columns:\n",
    "        c = c.replace(r'[', '')\n",
    "        c = c.replace(r']', '')\n",
    "        c = c.replace(r'.', '_')\n",
    "        c = c.replace(r'%', '')\n",
    "        new_columns.append(c)\n",
    "    Data_T.columns=new_columns\n",
    "    Data_T = Data_T.reset_index(drop = True)\n",
    "    Data_T = Data_T.astype({'clocks_applications_graphicsMHz':'float'})\n",
    "    Data_T = Data_T.astype({'clocks_applications_memoryMHz':'float'})\n",
    "    Data_T = Data_T.astype({'clocks_current_graphicsMHz':'float'})\n",
    "    Data_T = Data_T.astype({'clocks_current_memoryMHz':'float'})\n",
    "    Data_T = Data_T.astype({'clocks_max_graphicsMHz':'float'})\n",
    "    Data_T = Data_T.astype({'clocks_max_memoryMHz':'float'})\n",
    "    Data_T = Data_T.astype({'clocks_max_smMHz':'float'})\n",
    "    Data_T = Data_T.astype({'memory_freeMiB':'float'})\n",
    "    Data_T = Data_T.astype({'memory_totalMiB':'float'})\n",
    "    Data_T = Data_T.astype({'memory_usedMiB':'float'})\n",
    "    Data_T = Data_T.astype({'power_max_limitW':'float'})\n",
    "    Data_T = Data_T.astype({'power_min_limitW':'float'})\n",
    "\n",
    "    # 움직이는 매트릭값들 판별\n",
    "#     timeseries_columns = list(Data_T.columns)\n",
    "#     for i in (Data_T.columns):\n",
    "#         if( len(Data_T[i].value_counts()) == 1):\n",
    "#             timeseries_columns.remove(i)\n",
    "\n",
    "    for model in model_list: # 모델별로 실행\n",
    "        try:  # 해당모델없으면 에러무시하고 넘어가기\n",
    "            filenames = glob.glob(str(Instance)+'/'+str(model) )        \n",
    "            for file in filenames: # 해당 모델이름으로 필터링된 파일 모두살펴보기\n",
    "                print(file) #이 파일위주로 볼것이다.\n",
    "    \n",
    "                # 모델별로 epoch 시작과끝 기록한 파일열어서 array 에 저장\n",
    "                epoch_start_timestamp_array, epoch_end_timestamp_array, epoch_start_end_timestamp_Dataframe = hw_functions.per_model_csv(file)\n",
    "                print( '[총 에포크수]  : ' + str(  len(epoch_start_timestamp_array)))\n",
    "                \n",
    "                # epoch 시작 끝지점이 Data_T 랑 완전히 같지않기 때문에 가장 가까운 지점만 골라내줌 \n",
    "                epoch_start_timestamp_nearest, epoch_end_timestamp_nearest, epoch_start_list, epoch_end_list = hw_functions.epoch_start_end_to_list(Data_T, epoch_start_timestamp_array ,epoch_end_timestamp_array)\n",
    "                \n",
    "                # Data_T2 : 해당 모델 데이터만 가지고 살펴본다 \n",
    "                # 시작한지점이랑 끝난지점 기준으로 데이터 앞뒤 잘라내줌.\n",
    "                Data_T2 = Data_T[epoch_start_list[0]:epoch_end_list[len(epoch_end_list)-1]]\n",
    "\n",
    "                # 1th\n",
    "                # Instances / model / batchsize / datasize / totaldata / total_latency_second / one_epoch_average_second\n",
    "                Train_Name = Instance_name + '_' + file[file.rfind('/')+1:-4]\n",
    "                Instances = Instance_name\n",
    "                save_name = str(file[int(file.rfind('/')+1):-4]) \n",
    "                model =  str(save_name[:save_name.find('_batchsize')]) # MNIST_CNN예외처리\n",
    "                save_name = save_name[save_name.find('_batchsize')+1:]\n",
    "                batchsize =  str(save_name[save_name.find('batchsize')+9:save_name.find('_')])\n",
    "                save_name = save_name[save_name.find('_')+9:]\n",
    "                datasize =  str(save_name[:save_name.find('_')])\n",
    "                save_name = save_name[save_name.find('_')+1:]\n",
    "                totaldata =  str(save_name[save_name.rfind('_totaldata')+10:])\n",
    "\n",
    "                with open(file, 'rb') as fr:\n",
    "                    epoch_start_end_timestamp = pickle.load(fr)\n",
    "\n",
    "                    start = datetime.strptime(epoch_start_end_timestamp[0][0],'%Y-%m-%d %H:%M:%S.%f')\n",
    "                    end = datetime.strptime(epoch_start_end_timestamp[len(epoch_start_end_timestamp)-1][1],'%Y-%m-%d %H:%M:%S.%f')\n",
    "                    total_latency_second = end.timestamp() - start.timestamp()\n",
    "                    one_epoch_average_second = total_latency_second/10\n",
    "                \n",
    "                # 2th\n",
    "                describe_columns_value = [] \n",
    "                describe_columns=[]\n",
    "                column_name = []\n",
    "                one_hot_encoding_onlytwo_columns = []\n",
    "                one_hot_encoding_onlytwo_value = []\n",
    "\n",
    "                Data_T2_desribe_result= Data_T2.describe() \n",
    "                # describe 에서 뺴줌\n",
    "                del_describe = ['new_timestamp_gpu','accounting_buffer_size','clocks_max_graphicsMHz','clocks_max_memoryMHz','clocks_max_smMHz','pcie_link_gen_current','pcie_link_gen_max','pcie_link_width_current','pcie_link_width_max','count','index','inforom_ecc','inforom_oem','clocks_applications_graphicsMHz','clocks_applications_memoryMHz','clocks_current_memoryMHz','power_max_limitW','power_min_limitW','serial']                    \n",
    "                for d in del_describe:\n",
    "                    del Data_T2_desribe_result[d]  \n",
    "\n",
    "                d= Data_T2_desribe_result[1:].to_dict(orient='dict')\n",
    "                for key in d.keys():\n",
    "                    for subkey in d[key].keys():\n",
    "                        column_name = key +'_'+ subkey\n",
    "                        globals()[column_name] = d[key][subkey]\n",
    "                        describe_columns.append(column_name)\n",
    "                        describe_columns_value.append(globals()[column_name])\n",
    "                \n",
    "                \n",
    "                # 변화안하는 nvidia-smi 값도 넣어줌 'not_move_value_columns'\n",
    "                not_move_value_columns_values = []\n",
    "                for i in features.not_move_value_columns:\n",
    "                    not_move_value_columns_values.append(Data_T[i].value_counts().index[0])\n",
    "\n",
    "                # 위에껀 nvidia-smi 전부다\n",
    "                #columns_name_value = [Instances,model,batchsize,datasize,totaldata,total_latency_second,one_epoch_average_second] + describe_columns_value  + not_move_value_columns_values + one_hot_encoding_onlytwo_value                  \n",
    "                columns_name_value = [Train_Name,Instances,model,batchsize,datasize,totaldata,total_latency_second,one_epoch_average_second] + describe_columns_value  + not_move_value_columns_values                   \n",
    "                df.loc[df.shape[0]] = columns_name_value\n",
    "                print(\"END\")\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "devicequery_result = pd.read_csv('./devicequery_result.csv')\n",
    "devicequery_result = devicequery_result.drop(index=11, axis=0)  # 숫자아닌값 제외해줌\n",
    "\n",
    "for i in (list(devicequery_result['features'])):  # 열 붙이기\n",
    "    df[i] = 1\n",
    "for i in list(df['Instances'].value_counts().keys()):\n",
    "    for j in (list(devicequery_result['features'])):\n",
    "        print(j)\n",
    "        df_new = float( devicequery_result [devicequery_result['features'] == j][i])     \n",
    "        df.loc[df.Instances ==i,j]= df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./nvidiasmi_devicequery_all.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
